{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Model, Input to BERT Layer is Embedded (Not Tokens Ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFBertForMaskedLM\n",
    "from transformers import TFCamembertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters\n",
    "\n",
    "vocabSize = 32005\n",
    "batchSize = 1\n",
    "sequenceSize = 32\n",
    "hiddenDimension = 768\n",
    "outputDimension = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Check BERT output\n",
    "\n",
    "# bertLayer = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")\n",
    "# bertLayer(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### generate a random tensor\n",
    "# randTensor = tf.random.uniform(shape=[batchSize, sequenceSize, hiddenDimension])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Check BERT output\n",
    "\n",
    "# bertLayer(None, inputs_embeds=randTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = tf.keras.Input(shape=(32), dtype='int32')\n",
    "# x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(inp)[0]\n",
    "# x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "# out = tf.keras.layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "# model = tf.keras.Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = tf.keras.Input(shape=(sequenceSize, hiddenDimension), batch_size=batchSize, dtype='float32')\n",
    "# x = bertLayer(None, inputs_embeds=inp)[0]\n",
    "# x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "# out = tf.keras.layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "# model = tf.keras.Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(randTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Classic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Build the Classic Model\n",
    "\n",
    "# inp = tf.keras.Input(shape=(sequenceSize), dtype='int32')\n",
    "# x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(inp)[0]\n",
    "# x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "# out = tf.keras.layers.Dense(outputDimension)(x)\n",
    "\n",
    "# modelClassic = tf.keras.Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(modelClassic.layers))\n",
    "# print(len(modelClassic.variables))\n",
    "\n",
    "# # 4\n",
    "# # 206"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Model With a Linear Projection at the Beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Build a Linear Layer to Embed the tokens_ids\n",
    "\n",
    "# linearProj = tf.keras.layers.Dense(hiddenDimension, input_shape=(vocabSize,), use_bias=False)\n",
    "\n",
    "# # # compile layer\n",
    "# # linearProj.compile()  # dense object does not have attribute compile\n",
    "\n",
    "# # execute the layer  # executing the layer does not solve the issue\n",
    "# tensor = tf.zeros(shape=[batchSize, sequenceSize, vocabSize])\n",
    "# linearProj(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Build a BERT Layer\n",
    "\n",
    "# bertLayer = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")\n",
    "\n",
    "# # compile layer\n",
    "# bertLayer.compile()  # compiling the layer does not solve the issue\n",
    "\n",
    "# # execute the layer\n",
    "# randTensor = tf.random.uniform(shape=[batchSize, sequenceSize, hiddenDimension])\n",
    "# bertLayer(None, inputs_embeds=randTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Build the Experimental Model 0\n",
    "\n",
    "# # It does not work if batch_size is not specified.\n",
    "\n",
    "# inp = tf.keras.Input(shape=(sequenceSize, vocabSize), batch_size=batchSize, dtype='float32')\n",
    "# x = linearProj(inp)\n",
    "# x = bertLayer(None, inputs_embeds=x)[0]\n",
    "# x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "# out = tf.keras.layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "# modelExp = tf.keras.Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Build the Experimental Model 1\n",
    "\n",
    "# inpA = tf.keras.Input(shape=(sequenceSize), dtype='int32')\n",
    "# inpB = tf.keras.Input(shape=(sequenceSize, vocabSize), batch_size=batchSize, dtype='float32')\n",
    "# x = tf.keras.layers.Dense(hiddenDimension, input_shape=(vocabSize,), use_bias=False)(inpB)\n",
    "# x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(None, inputs_embeds=x)[0]\n",
    "# x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "# out = tf.keras.layers.Dense(outputDimension, activation='softmax')(x)\n",
    "\n",
    "# modelExp = tf.keras.Model(inputs=[inpA, inpB], outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # inputs = keras.Input(batch_shape=(batch_size, timesteps, input_dim))\n",
    "\n",
    "# inpA = tf.keras.Input(shape=(sequenceSize), dtype='int32')\n",
    "# inpB = tf.keras.Input(batch_shape=(batchSize, sequenceSize, vocabSize), dtype='float32')\n",
    "# x = tf.keras.layers.Dense(hiddenDimension, input_shape=(vocabSize,), use_bias=False)(inpB)\n",
    "# x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(None, inputs_embeds=x)[0]\n",
    "# x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "# out = tf.keras.layers.Dense(outputDimension, activation='softmax')(x)\n",
    "\n",
    "# modelExp = tf.keras.Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Build the Experimental Model 3\n",
    "\n",
    "# initialTensor = tf.zeros(shape=[batchSize, sequenceSize, vocabSize])\n",
    "\n",
    "# inpA = tf.keras.Input(shape=(sequenceSize), dtype='int32')\n",
    "# inpB = tf.keras.Input(batch_shape=(batchSize, sequenceSize, vocabSize), dtype='float32', tensor=initialTensor)\n",
    "# x = tf.keras.layers.Dense(hiddenDimension, input_shape=(vocabSize,), use_bias=False)(inpB)\n",
    "# x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(None, inputs_embeds=x)[0]\n",
    "# x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "# out = tf.keras.layers.Dense(outputDimension, activation='softmax')(x)\n",
    "\n",
    "# modelExp = tf.keras.Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Build the Experimental Model 4\n",
    "\n",
    "# inp = tf.keras.Input(batch_shape=(batchSize, sequenceSize, hiddenDimension), dtype='float32')\n",
    "# x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(None, inputs_embeds=inp)[0]\n",
    "# x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "# out = tf.keras.layers.Dense(outputDimension, activation='softmax')(x)\n",
    "\n",
    "# modelExp = tf.keras.Model(inp, out, name='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Build the Experimental Model 5\n",
    "\n",
    "# initialTensor = tf.Variable(initial_value=1., shape=tf.TensorShape(None))\n",
    "\n",
    "# inp = tf.keras.Input(batch_shape=(batchSize, sequenceSize, hiddenDimension), dtype='float32', tensor=initialTensor)\n",
    "# x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(None, inputs_embeds=inp)[0]\n",
    "# x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "# out = tf.keras.layers.Dense(outputDimension, activation='softmax')(x)\n",
    "\n",
    "# modelExp = tf.keras.Model(inp, out, name='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(modelExp.layers))\n",
    "# print(len(modelExp.variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build The Experimental Model And Execute It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Build the Experimental Model 0\n",
    "\n",
    "# inpA = tf.keras.Input(shape=(sequenceSize), dtype='int32')\n",
    "# x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(inpA)[0]\n",
    "# x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "# out = tf.keras.layers.Dense(outputDimension, activation='softmax')(x)\n",
    "\n",
    "# modelExp = tf.keras.Model(inpA, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build the Experimental Model 1\n",
    "\n",
    "inpA = tf.keras.Input(shape=(sequenceSize), dtype='int32')\n",
    "inpB = tf.keras.Input(shape=(sequenceSize, hiddenDimension), batch_size=batchSize, dtype='float32')\n",
    "x = TFCamembertForMaskedLM.from_pretrained(\"jplu/tf-camembert-base\")(inpA, custom_embeds=inpB)[0]\n",
    "x = tf.keras.layers.Reshape((sequenceSize*vocabSize,))(x)\n",
    "out = tf.keras.layers.Dense(outputDimension, activation='softmax')(x)\n",
    "\n",
    "modelExp = tf.keras.Model(inputs=[inpA, inpB], outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "206\n"
     ]
    }
   ],
   "source": [
    "print(len(modelExp.layers))\n",
    "print(len(modelExp.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32)\n",
      "(1, 32, 32005)\n",
      "(1, 32, 768)\n"
     ]
    }
   ],
   "source": [
    "### Test the Model on an Input Sentence\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jplu/tf-camembert-base\", do_lower_case=True)\n",
    "sentence = \"Elle se situe au cœur d'un vaste bassin sédimentaire aux sols fertiles et au climat tempéré Elle se situe au cœur d'un vaste bassin\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "sentenceTensor = tf.expand_dims(tf.convert_to_tensor(tokens_ids), 0)\n",
    "sentenceTensorOneHot = tf.one_hot(sentenceTensor, vocabSize)\n",
    "randTensor = tf.random.uniform(shape=[batchSize, sequenceSize, hiddenDimension])\n",
    "print(sentenceTensor.shape)\n",
    "print(sentenceTensorOneHot.shape)\n",
    "print(randTensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'139881316355712'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a805afa1895d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodelExp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentenceTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/.venvTrans/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/.venvTrans/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    715\u001b[0m                                 ' implement a `call` method.')\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m     return self._run_internal_graph(\n\u001b[0m\u001b[1;32m    718\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n",
      "\u001b[0;32m~/Documents/.venvTrans/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m           \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_map_tensor_if_from_keras_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/.venvTrans/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/.venvTrans/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/.venvTrans/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_map_tensor_if_from_keras_layer\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    880\u001b[0m                 hasattr(t, '_keras_history')):\n\u001b[1;32m    881\u001b[0m               \u001b[0mt_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m               \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '139881316355712'"
     ]
    }
   ],
   "source": [
    "modelExp(sentenceTensor, randTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvTrans",
   "language": "python",
   "name": "venvtrans"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
